{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f802e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yake \n",
    "import re, os, string\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ba9aacf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I am not satisfied with the boarding procedure and the flight attendant hasn't greeted me well\"]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "text=\"\"\"I am not satisfied with the boarding procedure and the flight attendant hasn't greeted me well\"\"\"\n",
    "tokenized_text=sent_tokenize(text)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "02232f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'not', 'satisfied', 'with', 'the', 'boarding', 'procedure', 'and', 'the', 'flight', 'attendant', 'has', \"n't\", 'greeted', 'me', 'well']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tokenized_word=word_tokenize(text)\n",
    "print(tokenized_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04b847ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'d', 'over', 'll', 'm', 'such', 'from', 'themselves', 're', \"you've\", 'i', 'be', 'on', \"wasn't\", 'ma', 'ain', 'my', 'needn', 'wouldn', 'have', 'but', 'after', 'own', 'there', \"should've\", 'yourself', 'down', 'other', 'haven', \"haven't\", 'am', 'because', 'hadn', \"couldn't\", 'being', 'their', 'again', 'with', 'below', 'yourselves', 'just', 'didn', 'some', 'it', 'doesn', 'hasn', 'all', 'myself', 'through', 'where', \"shouldn't\", 'out', \"needn't\", 'shan', 'not', \"that'll\", \"you're\", 'was', 'ourselves', 'above', 'did', 'why', 'nor', \"didn't\", \"doesn't\", \"hasn't\", 'then', 'when', 'his', 'them', 'further', 'so', \"mustn't\", 'any', 'me', 'ours', 'y', 'mightn', 'for', 'in', \"it's\", 'an', 'himself', 'as', 'does', \"she's\", \"won't\", 'you', 'before', 'is', 'most', \"aren't\", \"you'd\", 'no', 'this', 'had', 'to', 'wasn', 'we', 'were', 'can', 'they', 'by', 'up', 'between', 'that', 'very', \"weren't\", 'theirs', 'whom', 'both', 'your', \"hadn't\", 'shouldn', \"mightn't\", 'more', 'itself', 'don', 'and', 'of', 'mustn', \"wouldn't\", 'should', 'these', 's', 'a', 'now', 't', \"you'll\", 'o', 'its', 'about', 'do', 'at', 'won', 'couldn', \"isn't\", 'what', 'once', 'each', 'here', 'the', 'until', 'or', 'under', 'hers', 'too', 'than', 'will', 'if', 'weren', 'yours', 'only', 'are', 'herself', 'having', 'he', 'against', 've', 'aren', \"shan't\", 'during', 'who', 'few', 'into', 'off', 'her', 'our', 'those', 'been', 'him', 'she', 'how', 'same', \"don't\", 'has', 'doing', 'which', 'isn', 'while'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b149c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a41711b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Doc cleaning\"\"\"\n",
    "    \n",
    "    # Lowering text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Removing punctuation\n",
    "    text = \"\".join([c for c in text if c not in PUNCTUATION])\n",
    "    \n",
    "    # Removing whitespace and newlines\n",
    "    text = re.sub('\\s+',' ',text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c734757d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_coo(coo_matrix):\n",
    "    \"\"\"Sort a dict with highest score\"\"\"\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "\n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "    \n",
    "    #use only topn items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    "\n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "    \n",
    "    # word index and corresponding tf-idf score\n",
    "    for idx, score in sorted_items:\n",
    "        \n",
    "        #keep track of feature name and its corresponding score\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    "\n",
    "    #create a tuples of feature, score\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d6148ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords(vectorizer, feature_names, doc):\n",
    "    \"\"\"Return top k keywords from a doc using TF-IDF method\"\"\"\n",
    "\n",
    "    #generate tf-idf for the given document\n",
    "    tf_idf_vector = vectorizer.transform([doc])\n",
    "    \n",
    "    #sort the tf-idf vectors by descending order of scores\n",
    "    sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "\n",
    "    #extract only TOP_K_KEYWORDS\n",
    "    keywords=extract_topn_from_vector(feature_names,sorted_items,TOP_K_KEYWORDS)\n",
    "    \n",
    "    return list(keywords.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66a63504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "PUNCTUATION = \"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\" \n",
    "TOP_K_KEYWORDS = 3 # top k number of keywords to retrieve in a ranked document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfa64cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('survey.csv').head(5)\n",
    "#data.drop(['selected_text', 'sentiment'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7452be10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c7efcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(subset=['text'], inplace=True)\n",
    "data['text_clean'] = data['text'].apply(clean_text)\n",
    "corpora = data['text_clean'].to_list()\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b2f409a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kastu\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Initializing TF-IDF Vectorizer with stopwords\n",
    "vectorizer = TfidfVectorizer(stop_words=stop_words, smooth_idf=False, use_idf=True)\n",
    "\n",
    "# Creating vocab with our corpora\n",
    "# Exlcluding first 10 docs for testing purpose\n",
    "vectorizer.fit_transform(corpora)\n",
    "\n",
    "# Storing vocab\n",
    "feature_names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8f43758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text</th>\n",
       "      <th>top_keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the boarding procedure wasnt good didn’t like ...</td>\n",
       "      <td>[wasnt, wait, procedure]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>my baggage was handled poorly will not travel ...</td>\n",
       "      <td>[travel, poorly, handled]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the flight attendent was very rude and was not...</td>\n",
       "      <td>[rude, attendent, helpful]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the flight didn’t arrive on time and was time ...</td>\n",
       "      <td>[time, occasion, important]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the flight attendant was very helpful and i li...</td>\n",
       "      <td>[whole, liked, journey]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           full_text  \\\n",
       "0  the boarding procedure wasnt good didn’t like ...   \n",
       "1  my baggage was handled poorly will not travel ...   \n",
       "2  the flight attendent was very rude and was not...   \n",
       "3  the flight didn’t arrive on time and was time ...   \n",
       "4  the flight attendant was very helpful and i li...   \n",
       "\n",
       "                  top_keywords  \n",
       "0     [wasnt, wait, procedure]  \n",
       "1    [travel, poorly, handled]  \n",
       "2   [rude, attendent, helpful]  \n",
       "3  [time, occasion, important]  \n",
       "4      [whole, liked, journey]  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = []\n",
    "for doc in corpora[0:10]:\n",
    "    df = {}\n",
    "    df['full_text'] = doc\n",
    "    df['top_keywords'] = get_keywords(vectorizer, feature_names, doc)\n",
    "    result.append(df)\n",
    "    \n",
    "final = pd.DataFrame(result)\n",
    "final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48a11f8",
   "metadata": {},
   "source": [
    "# Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3d82336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keybert in c:\\users\\kastu\\appdata\\roaming\\python\\python39\\site-packages (0.5.1)\n",
      "Requirement already satisfied: rich>=10.4.0 in c:\\users\\kastu\\appdata\\roaming\\python\\python39\\site-packages (from keybert) (12.4.4)\n",
      "Requirement already satisfied: scikit-learn>=0.22.2 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from keybert) (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from keybert) (1.21.5)\n",
      "Requirement already satisfied: sentence-transformers>=0.3.8 in c:\\users\\kastu\\appdata\\roaming\\python\\python39\\site-packages (from keybert) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from rich>=10.4.0->keybert) (2.11.2)\n",
      "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in c:\\users\\kastu\\appdata\\roaming\\python\\python39\\site-packages (from rich>=10.4.0->keybert) (0.9.1)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from scikit-learn>=0.22.2->keybert) (1.7.3)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from scikit-learn>=0.22.2->keybert) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from scikit-learn>=0.22.2->keybert) (2.2.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\kastu\\appdata\\roaming\\python\\python39\\site-packages (from sentence-transformers>=0.3.8->keybert) (0.12.0)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\kastu\\appdata\\roaming\\python\\python39\\site-packages (from sentence-transformers>=0.3.8->keybert) (0.7.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (3.7)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\kastu\\appdata\\roaming\\python\\python39\\site-packages (from sentence-transformers>=0.3.8->keybert) (0.1.96)\n",
      "Requirement already satisfied: tqdm in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (4.64.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in c:\\users\\kastu\\appdata\\roaming\\python\\python39\\site-packages (from sentence-transformers>=0.3.8->keybert) (4.19.2)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\kastu\\appdata\\roaming\\python\\python39\\site-packages (from sentence-transformers>=0.3.8->keybert) (1.11.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (4.1.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\users\\kastu\\appdata\\roaming\\python\\python39\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (0.12.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (2022.3.15)\n",
      "Requirement already satisfied: filelock in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (3.6.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (6.0)\n",
      "Requirement already satisfied: requests in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (2.27.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (3.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers>=0.3.8->keybert) (0.4.4)\n",
      "Requirement already satisfied: click in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from nltk->sentence-transformers>=0.3.8->keybert) (8.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (2.0.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from torchvision->sentence-transformers>=0.3.8->keybert) (9.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --user keybert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "51da3243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keybert[flair] in c:\\users\\kastu\\appdata\\roaming\\python\\python39\\site-packages (0.5.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from keybert[flair]) (1.21.5)\n",
      "Requirement already satisfied: scikit-learn>=0.22.2 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from keybert[flair]) (1.0.2)\n",
      "Requirement already satisfied: rich>=10.4.0 in c:\\users\\kastu\\appdata\\roaming\\python\\python39\\site-packages (from keybert[flair]) (12.4.4)\n",
      "Requirement already satisfied: sentence-transformers>=0.3.8 in c:\\users\\kastu\\appdata\\roaming\\python\\python39\\site-packages (from keybert[flair]) (2.2.0)\n",
      "Collecting transformers==3.5.1\n",
      "  Downloading transformers-3.5.1-py3-none-any.whl (1.3 MB)\n",
      "Collecting keybert[flair]\n",
      "  Downloading keybert-0.5.0.tar.gz (19 kB)\n",
      "  Downloading keybert-0.4.0.tar.gz (18 kB)\n",
      "  Downloading keybert-0.3.0.tar.gz (17 kB)\n",
      "  Downloading keybert-0.2.0.tar.gz (12 kB)\n",
      "Collecting flair==0.7\n",
      "  Downloading flair-0.7-py3-none-any.whl (448 kB)\n",
      "Collecting sqlitedict>=1.6.0\n",
      "  Downloading sqlitedict-2.0.0.tar.gz (46 kB)\n",
      "Collecting sentencepiece<=0.1.91\n",
      "  Downloading sentencepiece-0.1.91.tar.gz (500 kB)\n",
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "Collecting bpemb>=0.3.2\n",
      "  Downloading bpemb-0.3.3-py3-none-any.whl (19 kB)\n",
      "Collecting mpld3==0.3\n",
      "  Downloading mpld3-0.3.tar.gz (788 kB)\n",
      "Requirement already satisfied: lxml in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from flair==0.7->keybert[flair]) (4.8.0)\n",
      "Requirement already satisfied: torch>=1.1.0 in c:\\users\\kastu\\appdata\\roaming\\python\\python39\\site-packages (from flair==0.7->keybert[flair]) (1.11.0)\n",
      "Requirement already satisfied: regex in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from flair==0.7->keybert[flair]) (2022.3.15)\n",
      "Requirement already satisfied: matplotlib>=2.2.3 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from flair==0.7->keybert[flair]) (3.5.1)\n",
      "Requirement already satisfied: segtok>=1.5.7 in c:\\users\\kastu\\appdata\\roaming\\python\\python39\\site-packages (from flair==0.7->keybert[flair]) (1.5.11)\n",
      "Collecting janome\n",
      "  Downloading Janome-0.4.2-py2.py3-none-any.whl (19.7 MB)\n",
      "Collecting hyperopt>=0.1.1\n",
      "  Downloading hyperopt-0.2.7-py2.py3-none-any.whl (1.6 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from flair==0.7->keybert[flair]) (2.8.2)\n",
      "Collecting gdown\n",
      "  Downloading gdown-4.4.0.tar.gz (14 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Collecting konoha<5.0.0,>=4.0.0\n",
      "  Downloading konoha-4.6.5-py3-none-any.whl (20 kB)\n",
      "Collecting deprecated>=1.2.4\n",
      "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: tabulate in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from flair==0.7->keybert[flair]) (0.8.9)\n",
      "Requirement already satisfied: tqdm>=4.26.0 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from flair==0.7->keybert[flair]) (4.64.0)\n",
      "Collecting gensim<=3.8.3,>=3.4.0\n",
      "  Downloading gensim-3.8.3.tar.gz (23.4 MB)\n",
      "Collecting ftfy\n",
      "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from bpemb>=0.3.2->flair==0.7->keybert[flair]) (2.27.1)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from deprecated>=1.2.4->flair==0.7->keybert[flair]) (1.12.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from gensim<=3.8.3,>=3.4.0->flair==0.7->keybert[flair]) (1.7.3)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from gensim<=3.8.3,>=3.4.0->flair==0.7->keybert[flair]) (1.16.0)\n",
      "Requirement already satisfied: smart_open>=1.8.1 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from gensim<=3.8.3,>=3.4.0->flair==0.7->keybert[flair]) (5.1.0)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from hyperopt>=0.1.1->flair==0.7->keybert[flair]) (2.0.0)\n",
      "Collecting py4j\n",
      "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
      "Requirement already satisfied: future in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from hyperopt>=0.1.1->flair==0.7->keybert[flair]) (0.18.2)\n",
      "Requirement already satisfied: networkx>=2.2 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from hyperopt>=0.1.1->flair==0.7->keybert[flair]) (2.7.1)\n",
      "Collecting importlib-metadata<4.0.0,>=3.7.0\n",
      "  Downloading importlib_metadata-3.10.1-py3-none-any.whl (14 kB)\n",
      "Collecting overrides<4.0.0,>=3.0.0\n",
      "  Downloading overrides-3.1.0.tar.gz (11 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair==0.7->keybert[flair]) (3.7.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair==0.7->keybert[flair]) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair==0.7->keybert[flair]) (3.0.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair==0.7->keybert[flair]) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair==0.7->keybert[flair]) (1.3.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair==0.7->keybert[flair]) (9.0.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair==0.7->keybert[flair]) (0.11.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from requests->bpemb>=0.3.2->flair==0.7->keybert[flair]) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from requests->bpemb>=0.3.2->flair==0.7->keybert[flair]) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from requests->bpemb>=0.3.2->flair==0.7->keybert[flair]) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from requests->bpemb>=0.3.2->flair==0.7->keybert[flair]) (1.26.9)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from scikit-learn>=0.22.2->keybert[flair]) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from scikit-learn>=0.22.2->keybert[flair]) (1.1.0)\n",
      "Collecting sentence-transformers>=0.3.8\n",
      "  Downloading sentence-transformers-2.1.0.tar.gz (78 kB)\n",
      "  Downloading sentence-transformers-2.0.0.tar.gz (85 kB)\n",
      "  Downloading sentence-transformers-1.2.1.tar.gz (80 kB)\n",
      "Requirement already satisfied: torchvision in c:\\users\\kastu\\appdata\\roaming\\python\\python39\\site-packages (from sentence-transformers>=0.3.8->keybert[flair]) (0.12.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert[flair]) (3.7)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from torch>=1.1.0->flair==0.7->keybert[flair]) (4.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from tqdm>=4.26.0->flair==0.7->keybert[flair]) (0.4.4)\n",
      "Requirement already satisfied: protobuf in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from transformers==3.5.1->keybert[flair]) (3.19.1)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from transformers==3.5.1->keybert[flair]) (3.6.0)\n",
      "Collecting tokenizers==0.9.3\n",
      "  Downloading tokenizers-0.9.3.tar.gz (172 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from ftfy->flair==0.7->keybert[flair]) (0.2.5)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from gdown->flair==0.7->keybert[flair]) (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from beautifulsoup4->gdown->flair==0.7->keybert[flair]) (2.3.1)\n",
      "Requirement already satisfied: click in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from nltk->sentence-transformers>=0.3.8->keybert[flair]) (8.0.4)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from requests->bpemb>=0.3.2->flair==0.7->keybert[flair]) (1.7.1)\n",
      "Building wheels for collected packages: mpld3, gensim, overrides, sentence-transformers, sentencepiece, sqlitedict, tokenizers, gdown, keybert, langdetect, sacremoses\n",
      "  Building wheel for mpld3 (setup.py): started\n",
      "  Building wheel for mpld3 (setup.py): finished with status 'done'\n",
      "  Created wheel for mpld3: filename=mpld3-0.3-py3-none-any.whl size=116702 sha256=822f8e8cbd6441f1c5ac8d74107c9967742f73a232e760de5037f5b93f3c3f2f\n",
      "  Stored in directory: c:\\users\\kastu\\appdata\\local\\pip\\cache\\wheels\\a6\\f4\\e6\\e40ff9021f6b3854af70fa8ea004f5ab95672817462df08fed\n",
      "  Building wheel for gensim (setup.py): started\n",
      "  Building wheel for gensim (setup.py): finished with status 'done'\n",
      "  Created wheel for gensim: filename=gensim-3.8.3-cp39-cp39-win_amd64.whl size=24150347 sha256=b4c1a47d870f23a66f6468b7863da8c6b9f6d294e358f9696b76d1e5dff2e88a\n",
      "  Stored in directory: c:\\users\\kastu\\appdata\\local\\pip\\cache\\wheels\\ca\\5d\\af\\618594ec2f28608c1d6ee7d2b7e95a3e9b06551e3b80a491d6\n",
      "  Building wheel for overrides (setup.py): started\n",
      "  Building wheel for overrides (setup.py): finished with status 'done'\n",
      "  Created wheel for overrides: filename=overrides-3.1.0-py3-none-any.whl size=10187 sha256=012d6c878d66305677ce1eea38d5576dd74227d11f182796597a257bfee8cc08\n",
      "  Stored in directory: c:\\users\\kastu\\appdata\\local\\pip\\cache\\wheels\\7d\\11\\0e\\73fdcb3d71d97e33c230900efe85923ee9d49515d050503174\n",
      "  Building wheel for sentence-transformers (setup.py): started\n",
      "  Building wheel for sentence-transformers (setup.py): finished with status 'done'\n",
      "  Created wheel for sentence-transformers: filename=sentence_transformers-1.2.1-py3-none-any.whl size=123301 sha256=18fb846078125bf82969a8644b74aac86d3743617523937384b8164b6c841a46\n",
      "  Stored in directory: c:\\users\\kastu\\appdata\\local\\pip\\cache\\wheels\\7d\\89\\99\\3ad94aa1a3101366137dabb0585fdac8339b3cdcee2108da82\n",
      "  Building wheel for sentencepiece (setup.py): started\n",
      "  Building wheel for sentencepiece (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for sentencepiece\n",
      "  Building wheel for sqlitedict (setup.py): started\n",
      "  Building wheel for sqlitedict (setup.py): finished with status 'done'\n",
      "  Created wheel for sqlitedict: filename=sqlitedict-2.0.0-py3-none-any.whl size=15733 sha256=06c61d27d046da834915721f2c8e67856f11c3e3e3c5bffb1d37a40803ae9512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ERROR: Command errored out with exit status 1:\n",
      "   command: 'C:\\Users\\kastu\\anaconda3\\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\kastu\\\\AppData\\\\Local\\\\Temp\\\\pip-install-m47w8hoz\\\\sentencepiece_2481b68efad24055a9c3261455666c34\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\kastu\\\\AppData\\\\Local\\\\Temp\\\\pip-install-m47w8hoz\\\\sentencepiece_2481b68efad24055a9c3261455666c34\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d 'C:\\Users\\kastu\\AppData\\Local\\Temp\\pip-wheel-upbutd7k'\n",
      "       cwd: C:\\Users\\kastu\\AppData\\Local\\Temp\\pip-install-m47w8hoz\\sentencepiece_2481b68efad24055a9c3261455666c34\\\n",
      "  Complete output (17 lines):\n",
      "  C:\\Users\\kastu\\anaconda3\\lib\\site-packages\\setuptools\\dist.py:757: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead\n",
      "    warnings.warn(\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-3.9\n",
      "  copying sentencepiece.py -> build\\lib.win-amd64-3.9\n",
      "  running build_ext\n",
      "  building '_sentencepiece' extension\n",
      "  creating build\\temp.win-amd64-3.9\n",
      "  creating build\\temp.win-amd64-3.9\\Release\n",
      "  \"C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.28.29910\\bin\\HostX86\\x64\\cl.exe\" /c /nologo /O2 /W3 /GL /DNDEBUG /MD -IC:\\Users\\kastu\\anaconda3\\include -IC:\\Users\\kastu\\anaconda3\\Include \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.28.29910\\include\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\ucrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\shared\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\um\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\winrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\cppwinrt\" /EHsc /Tpsentencepiece_wrap.cxx /Fobuild\\temp.win-amd64-3.9\\Release\\sentencepiece_wrap.obj /MT /I..\\build\\root\\include\n",
      "  cl : Command line warning D9025 : overriding '/MD' with '/MT'\n",
      "  sentencepiece_wrap.cxx\n",
      "  sentencepiece_wrap.cxx(2777): fatal error C1083: Cannot open include file: 'sentencepiece_processor.h': No such file or directory\n",
      "  error: command 'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio\\\\2019\\\\BuildTools\\\\VC\\\\Tools\\\\MSVC\\\\14.28.29910\\\\bin\\\\HostX86\\\\x64\\\\cl.exe' failed with exit code 2\n",
      "  ----------------------------------------\n",
      "  ERROR: Failed building wheel for sentencepiece\n",
      "  ERROR: Command errored out with exit status 1:\n",
      "   command: 'C:\\Users\\kastu\\anaconda3\\python.exe' 'C:\\Users\\kastu\\anaconda3\\lib\\site-packages\\pip\\_vendor\\pep517\\in_process\\_in_process.py' build_wheel 'C:\\Users\\kastu\\AppData\\Local\\Temp\\tmpo_dturw6'\n",
      "       cwd: C:\\Users\\kastu\\AppData\\Local\\Temp\\pip-install-m47w8hoz\\tokenizers_e48dc5209a214094b7f437047d61b258\n",
      "  Complete output (47 lines):\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-cpython-39\n",
      "  creating build\\lib.win-amd64-cpython-39\\tokenizers\n",
      "  copying py_src\\tokenizers\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\n",
      "  creating build\\lib.win-amd64-cpython-39\\tokenizers\\models\n",
      "  copying py_src\\tokenizers\\models\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\models\n",
      "  creating build\\lib.win-amd64-cpython-39\\tokenizers\\decoders\n",
      "  copying py_src\\tokenizers\\decoders\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\decoders\n",
      "  creating build\\lib.win-amd64-cpython-39\\tokenizers\\normalizers\n",
      "  copying py_src\\tokenizers\\normalizers\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\normalizers\n",
      "  creating build\\lib.win-amd64-cpython-39\\tokenizers\\pre_tokenizers\n",
      "  copying py_src\\tokenizers\\pre_tokenizers\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\pre_tokenizers\n",
      "  creating build\\lib.win-amd64-cpython-39\\tokenizers\\processors\n",
      "  copying py_src\\tokenizers\\processors\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\processors\n",
      "  creating build\\lib.win-amd64-cpython-39\\tokenizers\\trainers\n",
      "  copying py_src\\tokenizers\\trainers\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\trainers\n",
      "  creating build\\lib.win-amd64-cpython-39\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\base_tokenizer.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\bert_wordpiece.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\byte_level_bpe.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\char_level_bpe.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\sentencepiece_bpe.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\sentencepiece_unigram.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\__init__.pyi -> build\\lib.win-amd64-cpython-39\\tokenizers\n",
      "  copying py_src\\tokenizers\\models\\__init__.pyi -> build\\lib.win-amd64-cpython-39\\tokenizers\\models\n",
      "  copying py_src\\tokenizers\\decoders\\__init__.pyi -> build\\lib.win-amd64-cpython-39\\tokenizers\\decoders\n",
      "  copying py_src\\tokenizers\\normalizers\\__init__.pyi -> build\\lib.win-amd64-cpython-39\\tokenizers\\normalizers\n",
      "  copying py_src\\tokenizers\\pre_tokenizers\\__init__.pyi -> build\\lib.win-amd64-cpython-39\\tokenizers\\pre_tokenizers\n",
      "  copying py_src\\tokenizers\\processors\\__init__.pyi -> build\\lib.win-amd64-cpython-39\\tokenizers\\processors\n",
      "  copying py_src\\tokenizers\\trainers\\__init__.pyi -> build\\lib.win-amd64-cpython-39\\tokenizers\\trainers\n",
      "  running build_ext\n",
      "  running build_rust\n",
      "  error: can't find Rust compiler\n",
      "  \n",
      "  If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\n",
      "  \n",
      "  To update pip, run:\n",
      "  \n",
      "      pip install --upgrade pip\n",
      "  \n",
      "  and then retry package installation.\n",
      "  \n",
      "  If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\n",
      "  ----------------------------------------\n",
      "  ERROR: Failed building wheel for tokenizers\n",
      "ERROR: Could not build wheels for tokenizers which use PEP 517 and cannot be installed directly\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Stored in directory: c:\\users\\kastu\\appdata\\local\\pip\\cache\\wheels\\48\\a5\\80\\fa89dc26af0f4c280b500f5529978552379c1ce8907e0a281c\n",
      "  Building wheel for tokenizers (PEP 517): started\n",
      "  Building wheel for tokenizers (PEP 517): finished with status 'error'\n",
      "  Building wheel for gdown (PEP 517): started\n",
      "  Building wheel for gdown (PEP 517): finished with status 'done'\n",
      "  Created wheel for gdown: filename=gdown-4.4.0-py3-none-any.whl size=14759 sha256=5d11ed35ea59486a84501900890a740632d984084e530f6e1075c7fd4cc79b5f\n",
      "  Stored in directory: c:\\users\\kastu\\appdata\\local\\pip\\cache\\wheels\\7d\\37\\b6\\b2a79c75e898c0b8e46ff255102602d7159a10d9af0d80641a\n",
      "  Building wheel for keybert (setup.py): started\n",
      "  Building wheel for keybert (setup.py): finished with status 'done'\n",
      "  Created wheel for keybert: filename=keybert-0.2.0-py3-none-any.whl size=10611 sha256=a4d07d87025a4c2bfa63cd77861a882b37266a0f996bc682ae917186f66287d0\n",
      "  Stored in directory: c:\\users\\kastu\\appdata\\local\\pip\\cache\\wheels\\2b\\5e\\cb\\9bedeed618085f255420717d2960da9704821c46a9ffc1c3c3\n",
      "  Building wheel for langdetect (setup.py): started\n",
      "  Building wheel for langdetect (setup.py): finished with status 'done'\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=47c0fac63036032562244a09d91542be5377c325258ae23ea0605ffb2795e6e1\n",
      "  Stored in directory: c:\\users\\kastu\\appdata\\local\\pip\\cache\\wheels\\d1\\c1\\d9\\7e068de779d863bc8f8fc9467d85e25cfe47fa5051fff1a1bb\n",
      "  Building wheel for sacremoses (setup.py): started\n",
      "  Building wheel for sacremoses (setup.py): finished with status 'done'\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=5ec8c88fdf8653b37fb0005db8999d9cf08832758a05fb0b739e85a20047bcaf\n",
      "  Stored in directory: c:\\users\\kastu\\appdata\\local\\pip\\cache\\wheels\\12\\1c\\3d\\46cf06718d63a32ff798a89594b61e7f345ab6b36d909ce033\n",
      "Successfully built mpld3 gensim overrides sentence-transformers sqlitedict gdown keybert langdetect sacremoses\n",
      "Failed to build sentencepiece tokenizers\n"
     ]
    }
   ],
   "source": [
    "!pip install --user keybert[flair]\n",
    "!pip install keybert[flair]\n",
    "!pip install keybert[gensim]\n",
    "!pip install keybert[spacy]\n",
    "!pip install keybert[use]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "819421fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "\n",
    "doc = \"\"\"\n",
    "         My flight got delayed and no one has informed me the next steps\n",
    "\n",
    "\n",
    "\n",
    "      \"\"\"\n",
    "kw_model = KeyBERT()\n",
    "keywords = kw_model.extract_keywords(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2b8b3d02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('flight got delayed', 0.7702),\n",
       " ('got delayed informed', 0.6279),\n",
       " ('got delayed', 0.5387),\n",
       " ('delayed informed steps', 0.5295),\n",
       " ('delayed', 0.5278)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kw_model.extract_keywords(doc, keyphrase_ngram_range=(1, 3), stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c5a39613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('flight got delayed', 0.7702),\n",
       " ('got delayed informed', 0.6279),\n",
       " ('delayed informed steps', 0.5295)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kw_model.extract_keywords(doc, keyphrase_ngram_range=(3, 3), stop_words='english',\n",
    "                              use_mmr=True, diversity=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d8d4e27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "kw_model = KeyBERT(model='all-roberta-large-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4665465b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('flight got delayed', 0.6768),\n",
       " ('got delayed informed', 0.4217),\n",
       " ('flight got', 0.32),\n",
       " ('got delayed', 0.3187),\n",
       " ('delayed', 0.2975)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kw_model.extract_keywords(doc, keyphrase_ngram_range=(1, 3), stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "27383960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flair\n",
      "  Using cached flair-0.11.3-py3-none-any.whl (401 kB)\n",
      "Requirement already satisfied: transformers>=4.0.0 in c:\\users\\kastu\\appdata\\roaming\\python\\python39\\site-packages (from flair) (4.19.2)\n",
      "Requirement already satisfied: regex in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from flair) (2022.3.15)\n",
      "Collecting konoha<5.0.0,>=4.0.0\n",
      "  Using cached konoha-4.6.5-py3-none-any.whl (20 kB)\n",
      "Collecting conllu>=4.0\n",
      "  Using cached conllu-4.4.2-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: lxml in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from flair) (4.8.0)\n",
      "Collecting deprecated>=1.2.4\n",
      "  Using cached Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting gdown==4.4.0\n",
      "  Using cached gdown-4.4.0-py3-none-any.whl\n",
      "Requirement already satisfied: torch!=1.8,>=1.5.0 in c:\\users\\kastu\\appdata\\roaming\\python\\python39\\site-packages (from flair) (1.11.0)\n",
      "Collecting mpld3==0.3\n",
      "  Using cached mpld3-0.3-py3-none-any.whl\n",
      "Requirement already satisfied: tabulate in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from flair) (0.8.9)\n",
      "Collecting pptree\n",
      "  Using cached pptree-3.1-py3-none-any.whl\n",
      "Collecting hyperopt>=0.2.7\n",
      "  Using cached hyperopt-0.2.7-py2.py3-none-any.whl (1.6 MB)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\kastu\\appdata\\roaming\\python\\python39\\site-packages (from flair) (0.7.0)\n",
      "Requirement already satisfied: matplotlib>=2.2.3 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from flair) (3.5.1)\n",
      "Collecting janome\n",
      "  Using cached Janome-0.4.2-py2.py3-none-any.whl (19.7 MB)\n",
      "Collecting langdetect\n",
      "  Using cached langdetect-1.0.9-py3-none-any.whl\n",
      "Requirement already satisfied: sentencepiece==0.1.95 in c:\\users\\kastu\\appdata\\roaming\\python\\python39\\site-packages (from flair) (0.1.95)\n",
      "Requirement already satisfied: tqdm>=4.26.0 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from flair) (4.64.0)\n",
      "Collecting ftfy\n",
      "  Using cached ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
      "Collecting wikipedia-api\n",
      "  Using cached Wikipedia_API-0.5.4-py3-none-any.whl\n",
      "Collecting sqlitedict>=1.6.0\n",
      "  Using cached sqlitedict-2.0.0-py3-none-any.whl\n",
      "Collecting bpemb>=0.3.2\n",
      "  Using cached bpemb-0.3.3-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from flair) (2.8.2)\n",
      "Collecting more-itertools\n",
      "  Using cached more_itertools-8.13.0-py3-none-any.whl (51 kB)\n",
      "Requirement already satisfied: segtok>=1.5.7 in c:\\users\\kastu\\appdata\\roaming\\python\\python39\\site-packages (from flair) (1.5.11)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from flair) (1.0.2)\n",
      "Requirement already satisfied: gensim>=3.4.0 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from flair) (4.1.2)\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from gdown==4.4.0->flair) (2.27.1)\n",
      "Requirement already satisfied: six in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from gdown==4.4.0->flair) (1.16.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from gdown==4.4.0->flair) (3.6.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from gdown==4.4.0->flair) (4.11.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from bpemb>=0.3.2->flair) (1.21.5)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from deprecated>=1.2.4->flair) (1.12.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from gensim>=3.4.0->flair) (5.1.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from gensim>=3.4.0->flair) (1.7.3)\n",
      "Requirement already satisfied: future in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from hyperopt>=0.2.7->flair) (0.18.2)\n",
      "Collecting py4j\n",
      "  Using cached py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from hyperopt>=0.2.7->flair) (2.0.0)\n",
      "Requirement already satisfied: networkx>=2.2 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from hyperopt>=0.2.7->flair) (2.7.1)\n",
      "Collecting overrides<4.0.0,>=3.0.0\n",
      "  Using cached overrides-3.1.0-py3-none-any.whl\n",
      "Collecting importlib-metadata<4.0.0,>=3.7.0\n",
      "  Using cached importlib_metadata-3.10.1-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair) (3.7.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair) (21.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair) (4.25.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair) (1.3.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair) (9.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from requests[socks]->gdown==4.4.0->flair) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from requests[socks]->gdown==4.4.0->flair) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from requests[socks]->gdown==4.4.0->flair) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from requests[socks]->gdown==4.4.0->flair) (2.0.4)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from scikit-learn>=0.21.3->flair) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from scikit-learn>=0.21.3->flair) (2.2.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from torch!=1.8,>=1.5.0->flair) (4.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from tqdm>=4.26.0->flair) (0.4.4)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\users\\kastu\\appdata\\roaming\\python\\python39\\site-packages (from transformers>=4.0.0->flair) (0.12.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from transformers>=4.0.0->flair) (6.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from beautifulsoup4->gdown==4.4.0->flair) (2.3.1)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from ftfy->flair) (0.2.5)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from requests[socks]->gdown==4.4.0->flair) (1.7.1)\n",
      "Installing collected packages: py4j, overrides, importlib-metadata, wikipedia-api, sqlitedict, pptree, mpld3, more-itertools, langdetect, konoha, janome, hyperopt, gdown, ftfy, deprecated, conllu, bpemb, flair\n",
      "Successfully installed bpemb-0.3.3 conllu-4.4.2 deprecated-1.2.13 flair-0.11.3 ftfy-6.1.1 gdown-4.4.0 hyperopt-0.2.7 importlib-metadata-3.10.1 janome-0.4.2 konoha-4.6.5 langdetect-1.0.9 more-itertools-8.13.0 mpld3-0.3 overrides-3.1.0 pptree-3.1 py4j-0.10.9.5 sqlitedict-2.0.0 wikipedia-api-0.5.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script hyperopt-mongo-worker.exe is installed in 'C:\\Users\\kastu\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script gdown.exe is installed in 'C:\\Users\\kastu\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script ftfy.exe is installed in 'C:\\Users\\kastu\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sphinx 4.4.0 requires importlib-metadata>=4.4; python_version < \"3.10\", but you have importlib-metadata 3.10.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install --user flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b6d6baab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "from flair.embeddings import TransformerDocumentEmbeddings\n",
    "\n",
    "roberta = TransformerDocumentEmbeddings('roberta-base')\n",
    "kw_model = KeyBERT(model=roberta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d68b8cc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('flight got delayed', 0.9988),\n",
       " ('delayed', 0.9985),\n",
       " ('delayed informed', 0.9982)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kw_model.extract_keywords(doc, keyphrase_ngram_range=(1, 3), stop_words='english', top_n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9dcb39de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow>=2.0.0\n",
      "  Downloading tensorflow-2.9.1-cp39-cp39-win_amd64.whl (444.0 MB)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from tensorflow>=2.0.0) (3.6.0)\n",
      "Collecting flatbuffers<2,>=1.12\n",
      "  Using cached flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from tensorflow>=2.0.0) (3.19.1)\n",
      "Collecting tensorboard<2.10,>=2.9\n",
      "  Downloading tensorboard-2.9.0-py3-none-any.whl (5.8 MB)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from tensorflow>=2.0.0) (1.21.5)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.1.0-py3-none-any.whl (123 kB)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-14.0.1-py2.py3-none-win_amd64.whl (14.2 MB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from tensorflow>=2.0.0) (4.1.1)\n",
      "Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0\n",
      "  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.26.0-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "Collecting keras<2.10.0,>=2.9.0rc0\n",
      "  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from tensorflow>=2.0.0) (21.3)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from tensorflow>=2.0.0) (1.16.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from tensorflow>=2.0.0) (1.12.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from tensorflow>=2.0.0) (61.2.0)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Using cached termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from tensorflow>=2.0.0) (1.42.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow>=2.0.0) (0.37.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.0.0) (3.3.4)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.0.0) (2.0.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.0.0) (2.27.1)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.0.0) (1.33.0)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.0.0) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.0.0) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.0.0) (0.2.8)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.0.0) (0.4.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow>=2.0.0) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow>=2.0.0) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow>=2.0.0) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow>=2.0.0) (2021.10.8)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from packaging->tensorflow>=2.0.0) (3.0.4)\n",
      "Building wheels for collected packages: termcolor\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4848 sha256=94e0b7b4958c7ce47d59958bb43ce3a657559b4d72cf383ba9c465a787c56b61\n",
      "  Stored in directory: c:\\users\\kastu\\appdata\\local\\pip\\cache\\wheels\\b6\\0d\\90\\0d1bbd99855f99cb2f6c2e5ff96f8023fad8ec367695f7d72d\n",
      "Successfully built termcolor\n",
      "Installing collected packages: oauthlib, requests-oauthlib, tensorboard-plugin-wit, tensorboard-data-server, google-auth-oauthlib, absl-py, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, opt-einsum, libclang, keras-preprocessing, keras, google-pasta, gast, flatbuffers, astunparse, tensorflow\n",
      "Successfully installed absl-py-1.1.0 astunparse-1.6.3 flatbuffers-1.12 gast-0.4.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 keras-2.9.0 keras-preprocessing-1.1.2 libclang-14.0.1 oauthlib-3.2.0 opt-einsum-3.3.0 requests-oauthlib-1.3.1 tensorboard-2.9.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.9.1 tensorflow-estimator-2.9.0 tensorflow-io-gcs-filesystem-0.26.0 termcolor-1.1.0\n",
      "Collecting tensorflow-hub\n",
      "  Downloading tensorflow_hub-0.12.0-py2.py3-none-any.whl (108 kB)\n",
      "Requirement already satisfied: numpy>=1.12.0 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from tensorflow-hub) (1.21.5)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in c:\\users\\kastu\\anaconda3\\lib\\site-packages (from tensorflow-hub) (3.19.1)\n",
      "Installing collected packages: tensorflow-hub\n",
      "Successfully installed tensorflow-hub-0.12.0\n"
     ]
    }
   ],
   "source": [
    "! pip install \"tensorflow>=2.0.0\"\n",
    "! pip install --upgrade tensorflow-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "955ae9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub\n",
    "embedding_model = tensorflow_hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "kw_model = KeyBERT(model=embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7a5091f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\kastu\\AppData\\Roaming\\Python\\Python39\\site-packages\\keybert\\backend\\_use.py:55: _EagerTensorBase.cpu (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.identity instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\kastu\\AppData\\Roaming\\Python\\Python39\\site-packages\\keybert\\backend\\_use.py:55: _EagerTensorBase.cpu (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.identity instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('flight got delayed', 0.617),\n",
       " ('got delayed', 0.492),\n",
       " ('got delayed informed', 0.4698)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kw_model.extract_keywords(doc, keyphrase_ngram_range=(1, 3), stop_words='english', top_n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fbac1225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-05 20:17:57,054 https://flair.informatik.hu-berlin.de/resources/embeddings/token/en-fasttext-crawl-300d-1M.vectors.npy not found in cache, downloading to C:\\Users\\kastu\\AppData\\Local\\Temp\\tmp9x07jv4i\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 1200000128/1200000128 [01:01<00:00, 19450335.41B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-05 20:18:59,211 copying C:\\Users\\kastu\\AppData\\Local\\Temp\\tmp9x07jv4i to cache at C:\\Users\\kastu\\.flair\\embeddings\\en-fasttext-crawl-300d-1M.vectors.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-05 20:19:00,630 removing temp file C:\\Users\\kastu\\AppData\\Local\\Temp\\tmp9x07jv4i\n",
      "2022-06-05 20:19:01,206 https://flair.informatik.hu-berlin.de/resources/embeddings/token/en-fasttext-crawl-300d-1M not found in cache, downloading to C:\\Users\\kastu\\AppData\\Local\\Temp\\tmpkfd3fjv7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 39323680/39323680 [00:02<00:00, 13224993.62B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-05 20:19:04,614 copying C:\\Users\\kastu\\AppData\\Local\\Temp\\tmpkfd3fjv7 to cache at C:\\Users\\kastu\\.flair\\embeddings\\en-fasttext-crawl-300d-1M\n",
      "2022-06-05 20:19:04,646 removing temp file C:\\Users\\kastu\\AppData\\Local\\Temp\\tmpkfd3fjv7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from flair.embeddings import WordEmbeddings, DocumentPoolEmbeddings\n",
    "\n",
    "glove_embedding = WordEmbeddings('crawl')\n",
    "document_glove_embeddings = DocumentPoolEmbeddings([glove_embedding])\n",
    "\n",
    "kw_model = KeyBERT(model=document_glove_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1ebebbd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('flight got delayed', 0.7248),\n",
       " ('got delayed informed', 0.7067),\n",
       " ('delayed informed steps', 0.7062)]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kw_model.extract_keywords(doc, keyphrase_ngram_range=(1, 3), stop_words='english', top_n=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
